{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wieso LangChain und LangGraph\n",
    "\n",
    "Wieso gibt es diese Abstraktionen LangChain und LangGraph.\n",
    "\n",
    "Das hat vornehmlich historische Gründe.\n",
    "\n",
    "- LangChain ganz zu Anfang hat einfach nur versucht, einheitliche Interfaces für LLMs, VectorStores etc. bereit zu stellen und daraus vorgefertigte Chains zu bauen.\n",
    "- Damals gab es noch keine LCEL (Diese lustige Operatorschreibweise mit den Pipes |)\n",
    "- Dann kam die LCEL und aus Chains wurden Runnables.\n",
    "- Es war immer noch mit Schwierigkeiten verbunden, elegant den Datenfluss in einem Agenten zu steuern. Die LCEL war dafür einfach nicht gebaut, aber die Anforderungen gingen immer mehr in Richtung Agent.\n",
    "- Daher die Idee, speziell für agentische, zyklische Anwendungen neue Abstraktionen zu bauen.\n",
    "\n",
    "Nun leben die LCEL und LangGraph nebeneinander her. Man möchte meist beides in größeren Anwendungen benutzen. Die beiden Baukästen haben auch viele gemeinsame Schnittpunkte:\n",
    "\n",
    "- Das Callback-System\n",
    "- Das Config-Objekt\n",
    "- Ein Graph ist auch ein Runnable\n",
    "- Langsmith-Tracing\n",
    "\n",
    "Legacy-Chains im LangChain-Repo, die nicht mit LCEL oder LangGraph gebaut sind werden je nach Fleiß der Commnity schrittweise umgebaut. Die alten Chains werden dann nicht mehr gepflegt.\n",
    "\n",
    "Mit den schnell besser werdenden LLMs ändern sich die Möglichkeiten und die Anforderungen an Frameworks.\n",
    "\n",
    "Vielleicht sieht LangChain in zwei Jahren schon wieder ganz anders aus oder ist überhaupt nicht mehr das Tooling der Wahl.\n",
    "\n",
    "Damit muss man leben können. Aktuell findet LangChain allerdings sehr großen Anklang. Machen wir uns also die Mühe und schauen etwas unter die Motorhaube.\n",
    "\n",
    "### Die LCEL kennen wir schon:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import llm\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Hello friend! Please tell me a joke about {topic}\"\n",
    ")\n",
    "\n",
    "lcel_chain = prompt | llm() | StrOutputParser()\n",
    "\n",
    "print(lcel_chain.invoke({\"topic\": \"star treck\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nun wollen wir uns LangGraph anschauen\n",
    "\n",
    "Einige Konzepte sind schon bekannt. Wir gehen trotzdem noch einmal durch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wir bauen einen Prompt\n",
    "\n",
    "- Damit der Agent funktionieren kann, braucht er eine Art Notizzettel, was er alles schon probiert hat um eine sinnvolle Antwort zurück zu geben. Er notiert diese Schritte auf einem sogenannten Scratchpad.\n",
    "- Außerdem bekommt unser Agent noch einen Query-Parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    MessagesPlaceholder,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wir definieren ein Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "tools = [WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wir definieren den Agenten und den Agent-Executor von prebuilt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_agent_executor\n",
    "\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "\n",
    "\n",
    "def formatter(iterator):\n",
    "    for chunk in iterator:\n",
    "        for k, v in chunk.items():\n",
    "            yield f\"{k}: {v}\"\n",
    "\n",
    "\n",
    "agent_runnable = create_openai_functions_agent(llm(), tools, prompt)\n",
    "chain = create_agent_executor(agent_runnable, tools) | formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \"Write a very short summary about Angela Merkel. Use Wikipedia\"}\n",
    "for chunk in chain.stream(inputs):\n",
    "    print(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
