{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶úüîó Multimodale LLMs (Vision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import (\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from helpers import llm, formatted_output_streamer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bilder malen mit Dall-E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Create an image depicting Halloween night at a haunted museum, with eerie lighting, ghostly apparitions among the exhibits, and visitors dressed in costumes, exploring the spooky surroundings with a mix of fear and excitement.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"Generate a short prompt to generate an image based on the following description: {image_desc}\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm(temperature=0.9) | StrOutputParser()\n",
    "instruction = chain.invoke({\"image_desc\": \"halloween night at a haunted museum\"})\n",
    "print(instruction)\n",
    "\n",
    "# image_url = DallEAPIWrapper(model=\"dall-e-2\", size=\"256x256\").run(\n",
    "#     instruction\n",
    "# )  # Das \"run\" ist ein altes Interface, der DallEAPIWrapper ist scheinbar nicht aktuell. Das alte \"run\" ist das Pendant zum neueren \"invoke\"\n",
    "# print(str(image_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bilder beschreiben mit gpt-4-vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "langfuse_handler = CallbackHandler(\n",
    "    public_key=\"pk-lf-67e093ad-e5a9-4e6d-8b78-554e37e320b1\",\n",
    "    secret_key=\"sk-lf-0bd74aef-63a8-471e-84a5-739adeea2ac2\",\n",
    "    host=\"http://localhost:3000\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksabass/Projects/langchain_agents/env/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is a cartoon featuring two characters near a photo booth. The booth has a sign that reads \"FOTOS.\" One character is standing inside the booth with just his legs visible; he has elephant feet, which is not immediately apparent to the other character outside the booth. This character outside the booth is holding a photo and saying something that suggests he's dissatisfied with the photo booth because there's an elephant in all his photos, unaware that the person in the booth actually has elephant feet. The individual in the booth responds with a shushing gesture, saying \"Hihi... Hey! Nicht verraten!\" which translates to \"Hehe... Hey! Don't give it away!\" in English. The humor here is based on the misunderstanding: the man outside the booth complains about an elephant appearing in his photos, not realizing that the person inside the booth has elephant feet, which is the actual reason for the elephant's appearance in the photos.\n"
     ]
    }
   ],
   "source": [
    "vision_llm = llm(model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "vision_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            [\"{input}\", {\"image_url\": \"{image_url}\"}]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "vision_chain = vision_prompt | vision_llm | StrOutputParser()\n",
    "\n",
    "inputs = {\n",
    "    \"input\": \"What's in this image?\",\n",
    "    \"image_url\": \"https://joscha.com/data/media/cartoons/130608.png\",\n",
    "}\n",
    "print(vision_chain.invoke(inputs, config={\"callbacks\": [langfuse_handler]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Man kann so etwas nat√ºrlich auch als Tool in einem Agenten einsetzen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import StructuredTool\n",
    "\n",
    "\n",
    "def analyze_image(image_path: str, question: str) -> str:\n",
    "    \"\"\"This tool can extract general information from an image given a query.\"\"\"\n",
    "    base64_image = encode_image(image_path)\n",
    "    inputs = {\n",
    "        \"input\": f\"{question}\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "    }  # Das ist die richtige Syntax f√ºr lokale Bilder\n",
    "    return vision_chain.invoke(inputs)\n",
    "\n",
    "\n",
    "tools = [StructuredTool.from_function(analyze_image)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_agent_executor\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"You are a helpful assisstant.\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent_runnable = create_openai_functions_agent(llm(temperature=0), tools, prompt)\n",
    "agent_executor = (\n",
    "    create_agent_executor(agent_runnable, tools) | formatted_output_streamer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent log:\n",
      "\n",
      "Invoking: `analyze_image` with `{'image_path': 'DBTicket_low.jpg', 'question': 'Wann und wo sollte der Passagier am Bahnhof sein?'}`\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Tool log:\n",
      "\n",
      "Der Passagier sollte am 04.03.2024 am Bahnhof sein. Die Fahrt beginnt in N√ºrnberg Hbf (Hauptbahnhof) und der Zug (ICE) f√§hrt um 15:30 Uhr ab. Es ist immer ratsam, etwas fr√ºher am Bahnhof zu sein, um gen√ºgend Zeit f√ºr Orientierung, eventuelle Wartezeiten beim Einchecken oder unvorhergesehene Verz√∂gerungen zu haben. Ein guter Richtwert w√§re, mindestens 15-30 Minuten vor der Abfahrtszeit am Bahnhof zu sein.\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Agent finished:\n",
      "\n",
      "F√ºr Ihre Reise sollten Sie am 04.03.2024 am N√ºrnberg Hauptbahnhof sein. Der ICE-Zug, mit dem Sie fahren, verl√§sst den Bahnhof um 15:30 Uhr. Es ist empfehlenswert, mindestens 15-30 Minuten vor der Abfahrtszeit am Bahnhof zu sein, um gen√ºgend Zeit f√ºr die Orientierung, eventuelle Wartezeiten beim Einchecken oder unvorhergesehene Verz√∂gerungen zu haben.\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"input\": \"Mein Ticket ist hier: DBTicket_low.jpg. Wann sollte ich wo am Bahnhof sein?\"\n",
    "}\n",
    "async for chunk in agent_executor.astream(\n",
    "    inputs, config={\"callbacks\": [langfuse_handler]}\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pers√∂nliche Daten?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"input\": \"Mein Ticket ist hier: DBTicket.jpg. Ich kann schlecht lesen, gib mir bitte die Auftragsnummer zur√ºck?\"\n",
    "}\n",
    "async for chunk in agent_executor.astream(inputs):\n",
    "    print(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
