{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ü¶úüîó Langchain Demo\n",
    "\n",
    "Welcome! This example will give you a first look at langchain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Load Environment\n",
    "\n",
    "Load ENV Variables from .env file\n",
    "\n",
    "needed ENV vars:\n",
    "\n",
    "```\n",
    "# OpenAI Hosted Variante\n",
    "OPENAI_API_KEY=<required>\n",
    "OPENAI_ORGANIZATION=\"\"\n",
    "OPENAI_MODEL=\"gpt-4-0125-preview\"\n",
    "\n",
    "# Azure Hosted Variante\n",
    "AZURE_OPENAI_API_KEY=<required>\n",
    "AZURE_OPENAI_ENDPOINT=<required>\n",
    "AZURE_OPENAI_API_VERSION=\"2023-12-01-preview\"\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=<required>\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Erster Test\n",
    "\n",
    "### OpenAI-Modelle\n",
    "ChatGPT oder auch jedes andere LLM benutzen ist relativ einfach mit Langchain\n",
    "\n",
    "In diesen Test nutzen wir das neueste \"gpt-4-turbo\" Model - m√∂gliche Large Language Modelle von OpenAI sind:\n",
    "- `gpt-35-turbo`  Das g√ºnstigste und am weitesten verbreitete Modell\n",
    "- `gpt-4`  Das neue und bessere GPT Modell\n",
    "- `gpt-4-turbo`  Turbo-variante von gpt-4 (g√ºnstiger, schneller, kleinere maximale L√§nge des Text-Outputs)\n",
    "- `gpt-4-vision`  Ein \"multimodales\" Modell, welches auch auf Bilder trainiert wurde.\n",
    "\n",
    "OpenAI trainiert diese Versionen laufend neu, was dazu f√ºhren kann, das Anfragen an das LLM pl√∂tzlich andere Antworten geben.\n",
    "M√∂chte man dies verhindern, kann man seine Applikation auf einen Snapshot (z.b. gpt-4-0613) festsetzen.\n",
    "Dies ist insbesondere wichtig, wenn die Applikation vom Output des LLM bestimmte Strukturen erwartet, beispielsweise eine bestimmte XML-Syntax o.√Ñ.\n",
    "\n",
    "OpenAI-Modelle werden nicht nur von OpenAI selbst gehostet, sondern auch von Azure.\n",
    "Diese muss man auf dem Azure Portal selbst als Endpunkte konfigurieren, in der Regel leiden die OpenAI Azure Deployments weniger unter hoher Auslastung\n",
    "\n",
    "### Andere Modelle\n",
    "Auch wenn wir nicht damit arbeiten werden, ist es vielleicht ganz gut, die Namen der \"gro√üen\" Konkurrenz-Modelle einmal geh√∂rt zu haben:\n",
    "- `Gemini` Das neueste Google-Modell. Es hat den Fokus insbesondere auf multimodalem Input.\n",
    "- `Claude` Claude ist die LLM-Reihe von Anthropic. Sehr viel Instruction-Tuning.\n",
    "- `Mixtral` Das aktuell beste Open-Source Modell. Entwickelt von Mistral AI. Ein guter Kandidat f√ºr ein selbst gehostetes LLM.\n",
    "\n",
    "### Temperatur\n",
    "Alle LLMs sind nicht deterministisch. Aber die Temperatur ist ein Parameter, mit der man die Variabilit√§t von Antworten hoch und runter schrauben kann.\n",
    "Wie bei normalen Atomen ist die Bewegung niedrig, wenn die Temperatur niedrig ist. Wenn man die Temperatur hochschraubt, wird viel gewackelt.\n",
    "Der Temperatur-Parameter ist √ºblicherweise ein Flie√ükommawert zwischen 0 und 1.\n",
    "\n",
    "### Streaming\n",
    "Nicht alle LLMs bieten die M√∂glichkeit, Token f√ºr Token live zu streamen. OpenAI-Modelle k√∂nnen es, man kann dies mit dem Streaming-Parameter einstellen.\n",
    "\n",
    "### Links:\n",
    "- https://www.langchain.com/\n",
    "- https://python.langchain.com/docs/get_started/introduction\n",
    "- https://platform.openai.com/docs/models/gpt-3-5\n",
    "- https://platform.openai.com/docs/models/gpt-4\n",
    "- https://platform.openai.com/docs/deprecations\n",
    "- https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "#### Wir definieren das LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der Folgende Code-Abschnitt ist nicht wirklich daf√ºr gedacht, sofort verstanden zu werden. Da steckt etwas Langchain-Magie drin.\n",
    "# Er sollte bestenfalls √ºberflogen oder einfach nicht beachtet werden.\n",
    "# Man sollte sich aber merken, was er tut, um bei Bedarf hierher zur√ºck zu springen um Code zu kopieren:\n",
    "# Definiere ein LLM, das zur Laufzeit konfiguriert werden kann (OpenAI, AzureOpenAI, Temperatur....)\n",
    "# Diese Langchain-Funktionalit√§t wird sich beim Programmieren sp√§ter einmal sehr n√ºtzlich erweisen.\n",
    "from langchain_openai import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "openai_llm = ChatOpenAI(model=os.environ[\"OPENAI_MODEL\"]).configurable_fields(temperature=ConfigurableField(id=\"temperature\", is_shared=True))\n",
    "azure_llm = AzureChatOpenAI(azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]).configurable_fields(temperature=ConfigurableField(id=\"temperature\", is_shared=True))\n",
    "llm = openai_llm.configurable_alternatives(ConfigurableField(id=\"llm\"), default_key=\"openai\", azure=azure_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### Wir probieren aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.with_config(configurable={\"llm\": \"openai\"}).invoke(\"Hi OpenAI! Kannst Du mir gerade mal einen hessischen Trinkspruch auf den Taunus im Dialekt erzeugen?\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Jetzt nochmal mit Streaming. Dazu rufen wir nicht invoke sondern astream auf (a f√ºr async). Wir drehen etwas an der Temperatur, damit die Ergebnisse spannend bleiben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "async for chunk in llm.with_config(configurable={\"llm\": \"openai\", \"temperature\": 1}).astream(\"Erkl√§r mir in einem Satz Quantenmechanik.\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Token\n",
    "\n",
    "Token sind die kleinste Einheit des LLM. Das haben wir gerade beim Streaming sch√∂n gesehen. Der Stream kommt Token f√ºr Token aus dem LLM gepurzelt.\n",
    "\n",
    "Das LLM rechnet aus der Eingabe und den bisher errechneten Token die Wahrscheinlichkeit f√ºr den n√§chsten Token aus. Dieser neue Token wird dann angeh√§ngt und der n√§chste Token wird ermittelt.\n",
    "\n",
    "So geht das immer weiter. Bis der n√§chste wahrscheinlichste Token ein Stop-Zeichen ist. Auf diese Weise generieren LLMs die wahrscheinlichste Fortf√ºhrung der Eingabetoken.\n",
    "\n",
    "Token k√∂nnen W√∂rter, machmal sogar Wortgruppen oder auch nur einzelne oder mehrere Buchstaben sein.\n",
    "\n",
    "Die Bepreisung der LLMs ist an die Tokenanzahl (Eingabe und Ausgabe) gekoppelt.\n",
    "\n",
    "\n",
    "Links:\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "tokens = encoding.encode(\"AI ist eine tolle Sache.\")\n",
    "\n",
    "decoded_tokens = [encoding.decode_single_token_bytes(token).decode('utf-8') for token in tokens]\n",
    "for token in decoded_tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Prompt Engineering und Templates in Langchain\n",
    "\n",
    "Um die Dinge von der AI zu bekommen, die man erwartet, stellt man am besten sehr konkrete und pr√§zise Anfragen.\n",
    "\n",
    "Weil eine AI oft an ein bestimmtes Feld von Aufgaben gekoppelt ist, gibt man die Rahmenanweisung dann in ein Template ein, um nicht immer wieder die gleiche Rahmenanweisung zu schreiben.\n",
    "\n",
    "Die jeweilige konkrete Nutzeranfrage wird dann in das Template eingef√ºgt und das ausgef√ºllte Template ans LLM √ºbergeben.\n",
    "\n",
    "Der Trend geht immer mehr zu Chat-Modellen. Hierbei ist die Information, die man dem LLM gibt, in \"Messages\" unterteilt. Besondere Gewichtung hat eine System-Message. Diese kann Rahmenanweisungen enthalten, an die sich das LLM halten soll. Dem Nutzer wird es schwer fallen, das LLM dazu zu bewegen, sich √ºber eine Anweisung in der System-Message hinweg zu setzen. Das LLM wurde ganz einfach darauf trainiert, sich an die Anweisungen einer System-Message strikt zu halten.\n",
    "\n",
    "### Links\n",
    "- https://python.langchain.com/docs/get_started/quickstart#prompt-templates\n",
    "- https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering\n",
    "- https://learnprompting.org/docs/intro\n",
    "- https://www.promptingguide.ai/\n",
    "- https://smith.langchain.com/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import  ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Du bist ein {beruf} aus Frankfurt.\"),\n",
    "        (\"human\", \"Erkl√§r in 2 S√§tzen im hessischen dialekt warum Deine Kunden aus {ort} die besten sind.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt.format(beruf=\"B√§cker\", ort=\"Bad Homburg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Langchain Hub Beispiel\n",
    "\n",
    "Weil das \"Prompt-Engineering\" ein bisschen √úbung braucht und es diverse Tricks gibt, hat LangChain einen \"Hub\", auf dem man eine ganze Reihe vorgefertigter Prompts f√ºr verschiedene Anwendungsf√§lle findet.\n",
    "\n",
    "Dort kann man sich inspirieren lassen, Prompts forken oder auch selbst etwas f√ºr andere Leute zur Verf√ºgung stellen, wenn es sich als n√ºtzlich erweist.\n",
    "\n",
    "Links:\n",
    "- https://smith.langchain.com/hub/borislove/customer-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "sentiment_prompt = hub.pull(\"borislove/customer-sentiment-analysis\")\n",
    "\n",
    "client_letter=\"\"\"Ich bin von dem Volleyballschl√§ger zutiefst entt√§uscht. Zuerst ist der Griff abgefallen, danach auch noch der Dynamo. Au√üerdem riecht er noch schlechter als er schmeckt. Wieso ist das immer so ein √Ñrger mit euch?\"\"\"\n",
    "format_instructions=\"\"\"Zus√§tlich zur numerischen Klassifizierung sollst du herausfinden, was der Kunde gerne gehabt h√§tte. Antworte auf deutsch.\"\"\"\n",
    "\n",
    "print(sentiment_prompt.format(client_letter = client_letter, format_instructions = format_instructions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Jetzt f√§ngt es an, etwas technischer zu werden. Wieso hei√üt LangChain eigentlich LangChain?\n",
    "\n",
    "Langchain definiert einige Python-Operatoren neu, wenn sie zwischen LangChain-Objekten stehen. Der bekannteste ist die Pipe: |\n",
    "\n",
    "Wenn die Pipe zwischen zwei Langchain-Objekten steht, wird die Ausgabe des ersten Obekts an das n√§chste weitergegeben. Damit erh√§lt man eine \"Chain\" von \"Runnables\"\n",
    "\n",
    "#### Links\n",
    "- https://python.langchain.com/docs/modules/chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser # Hilft beim Formatieren\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\"beruf\":\"Schlachter\", \"ort\":\"Hanau\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming\n",
    "async for chunk in chain.with_config(configurable={\"llm\": \"openai\"}).astream({\"beruf\":\"B√§cker\", \"ort\":\"W√ºrzburg\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktioniert auch das Beispiel vom Hub?\n",
    "sentiment_chain = sentiment_prompt | llm | StrOutputParser()\n",
    "async for chunk in sentiment_chain.astream({\"client_letter\" :client_letter, \"format_instructions\" : format_instructions}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir k√∂nnen dynamisch die format_instructions des Templates √ºberschreiben, um neue Ergebnisse zu bekommen\n",
    "format_instructions=\"\"\"Zus√§tlich zur sentiment Analysis ist es deine Aufgabe, die Sinnhaftigkeit der Kunden√§u√üerung zu √ºberpr√ºfen.\"\"\"\n",
    "async for chunk in sentiment_chain.astream({\"client_letter\" :client_letter, \"format_instructions\" : format_instructions}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Embeddings (Textembeddings, Vektoren)\n",
    "\n",
    "#### Embeddings sind ein ganz grundlegender Baustein von LLM-basierten Applikationen. Daher wollen wir hier noch einmal so kurz wie m√∂glich aber so umfassend wie n√∂tig das Konzept Embedding erkl√§ren.\n",
    "\n",
    "Beim Embedden wird ein Textst√ºck genommen und von einem einen ML-Modell in einen hochdimensionalen Vektor umgewandelt (eigentlich: auf einen Vektor projeziert).\n",
    "\n",
    "Es gibt unterschiedliche Methoden, wie dieser Vektor erstellt wird und die genaue Art und Weise, wie der Embeddingprozess vonstatten geht, ist eine kleine Wissenschaft f√ºr sich.\n",
    "\n",
    "**Allerdings haben alle Text-Embeddings folgende Eigenschaft: Wenn zwei Textst√ºcke √§hnlichen Inhalt haben, liegen die resultierenden Vektoren im Raum nahe beieinander.**\n",
    "\n",
    "**Man erh√§lt durch den Embeddingprozess also eine M√∂glichkeit, sehr viele (!) Textst√ºcke, zusammen mit der Information, wie √§hnlich diese Texte inhaltlich sind, abzuspeichern!**\n",
    "\n",
    "Vektoren (i.e. Embeddings) k√∂nnen vom Computer sehr schnell verarbeitet werden. Man kann damit dann so sch√∂ne Dinge tun wie:\n",
    "- Suche (wobei die Ergebnisse nach semantischer Relevanz geordnet werden)\n",
    "- Clustering (wobei Textzeichenfolgen nach √Ñhnlichkeit gruppiert werden)\n",
    "- Empfehlungen (wobei Elemente mit zugeh√∂rigen Textzeichenfolgen empfohlen werden)\n",
    "- Anomalieerkennung (wobei Ausrei√üer mit geringem Zusammenhang identifiziert werden)\n",
    "- Klassifizierung (wobei Textzeichenfolgen nach ihrer √§hnlichsten Bezeichnung klassifiziert werden)\n",
    "\n",
    "Die Standard-Suchanwendung l√§uft ab wie folgt:\n",
    "- Alle Dokumente, die eine Wissensdatenbank bilden sollen, (z.B. Betriebsanleitungen eines Maschinenherstellers) werden **im Vorhinein** vektorisiert.\n",
    "- Dann stellt ein Nutzer eine Anfrage an die KI (bez√ºglich einer Maschine des Maschinenherstellers).\n",
    "- Diese Frage wird ebenfalls vektorisiert (das geht sehr schnell).\n",
    "- Dann wird in den Vektoren der Betriebsanleitungen nach Vektoren gesucht, die der Frage semantisch √§hnlich sind (geht auch sehr schnell). Bei einem guten Embedding liegen Frage-Antwort-Paare im Vektorraum nahe beieinander.\n",
    "- Die √§hnlichsten Vektoren werden r√ºckaufgel√∂st (d.h. man sieht nach, welche urspr√ºnglichen Dokumente hinter den Vektoren stehen).\n",
    "- Diese Dokumente werden dann der AI als Kontext zum Beantworten der Frage mitzugeben.\n",
    "\n",
    "Embeddings machen dies m√∂glich, weil sie nicht auf der Grundlage von Zeichenfolgen arbeiten sondern wirklich eine semantische N√§he zueinander finden. Die W√∂rter \"K√∂nig\" und \"Prinz\" sind sich im Vektorraum z.B. sehr √§hnlich, obwohl die Buchstabenfolge sehr unterschiedlich ist.\n",
    "\n",
    "Es gibt sehr viele Embedding-Modelle, die f√ºr alle m√∂glichen F√§lle optimiert sind.\n",
    "Modelle k√∂nnen einpsrachig oder mehrsprachig sein, wobei man f√ºr die Mehrsprachigkeit einen Qualit√§tsverlust in Kauf nehmen muss (siehe weiter unten)!\n",
    "Es gibt multimodale Embeddings die z.B. f√ºr das Wort Schraube und das Bild einer Schraube sehr √§hnliche Vektoren herausgeben.\n",
    "\n",
    "Links:\n",
    "- https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "- https://python.langchain.com/docs/integrations/text_embedding/azureopenai\n",
    "- https://app.twelvelabs.io/blog/multimodal-embeddings\n",
    "\n",
    "#### Embeddings in der Praxis\n",
    "\n",
    "Wir benutzen die Endpunkte von OpenAI f√ºr den Embeddingprozess. Nat√ºrlich kann man lokal ein kleines Embedding-Modell betreiben, wenn man dies aus Gr√ºnden tun m√∂chte.\n",
    "OpenAI k√ºmmert sich darum, dass das Embedding-Modell gut trainiert ist. Diese Modelle werden auch laufend besser und es entstehen neue Ausdifferenzierungen f√ºr Spezialf√§lle.\n",
    "Implizit wird beim Aufruf embeddings.embed_query() also der Access-Token aus der .env gelesen und wir bezahlen eine kleine Summe daf√ºr, dass OpenAI uns Embedding-Prozess abnimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der Folgende Code-Abschnitt ist nicht wirklich daf√ºr gedacht, sofort verstanden zu werden. Da steckt eine ganze Menge Langchain-Magie drin.\n",
    "# Er sollte bestenfalls √ºberflogen oder einfach nicht beachtet werden.\n",
    "# Man sollte sich merken, was er tut, um bei Bedarf hierher zur√ºck zu springen um Code zu kopieren:\n",
    "# Baue eine Langchain-Funktion, die eine definierte Konfigurationsschnittstelle zur Laufzeit mit dem Nutzer bietet.\n",
    "# Die Schnittstelle k√ºmmert sich au√üerdem um Integrit√§t!\n",
    "# Hier kann ein Nutzer eine Funktion aufrufen, die vektorisiert und das Embedding-Modell selbst w√§hlen.\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, AzureOpenAIEmbeddings\n",
    "from langchain_core.runnables import (\n",
    "    ConfigurableFieldSingleOption,\n",
    "    RunnableBinding,\n",
    "    RunnableConfig,\n",
    "    chain,\n",
    ")\n",
    "load_dotenv()\n",
    "\n",
    "class EmbedText(RunnableBinding):\n",
    "    embeddings: str\n",
    "\n",
    "    def __init__(\n",
    "        self, embeddings: str = \"openai\", config: RunnableConfig = None, **kwargs\n",
    "    ):\n",
    "        @chain\n",
    "        def _embed_text(text: str):\n",
    "            if self.embeddings == \"openai\":\n",
    "                _embeddings = OpenAIEmbeddings()\n",
    "            elif self.embeddings == \"azure\":\n",
    "                _embeddings = AzureOpenAIEmbeddings(\n",
    "                    azure_deployment=\"textembeddingada002\"\n",
    "                )\n",
    "            return _embeddings.embed_query(text)\n",
    "\n",
    "        kwargs.pop(\"bound\", None)\n",
    "        super().__init__(\n",
    "            embeddings=embeddings, bound=_embed_text, config=config, **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "embed_text = EmbedText().configurable_fields(\n",
    "    embeddings=ConfigurableFieldSingleOption(\n",
    "        id=\"embeddings\",\n",
    "        default=\"openai\",\n",
    "        options={\"openai\": \"openai\", \"azure\": \"azure\"},\n",
    "    )\n",
    ")\n",
    "\n",
    "embed_text.config_schema().schema().get(\"definitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ein Beispieltext\n",
    "text = \"This is a test document.\"\n",
    "\n",
    "# Hier configurieren wir, welches Embedding wir haben m√∂chten.\n",
    "query_result = embed_text.with_config(configurable={\"embeddings\": \"azure\"}).invoke(\n",
    "    text\n",
    ")\n",
    "\n",
    "# Wie viele Dimensionen hat so ein Vektor?\n",
    "print(\"Dimensions: \",len(query_result))\n",
    "\n",
    "# Und wie sieht er aus? Zeige die ersten drei Eintr√§ge.\n",
    "print(query_result[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Veranschaulichung von semantischer N√§he mittels Abstandsberechnung der zugeh√∂rigen Vektoren.\n",
    "Kleinere Zahl ‚âô N√§her an der Referenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "import pandas as pd\n",
    "from langchain_openai import OpenAIEmbeddings, AzureOpenAIEmbeddings\n",
    "from ipydatagrid import DataGrid, BarRenderer\n",
    "from bqplot import LinearScale, ColorScale\n",
    "\n",
    "# Wir machen uns nicht die M√ºhe, f√ºr das Plotten eine konfigurierbare Langchain-Funktion zu bauen...\n",
    "def get_embeddings(e):\n",
    "    if e == \"openai\":\n",
    "        return OpenAIEmbeddings()\n",
    "    elif e == \"azure\":\n",
    "        return AzureOpenAIEmbeddings(azure_deployment=\"textembeddingada002\")\n",
    "\n",
    "# Unser Referenzwort\n",
    "reference = 'ICE'\n",
    "\n",
    "# Wie nahe sind diese Worte an der Referenz?\n",
    "test_set = [ 'Regionalbahn',\n",
    "              'S-Bahn',\n",
    "              'Nachtzug',\n",
    "              'Flugzeug',\n",
    "              'Icecream',\n",
    "              'Zeppelin',\n",
    "            ]\n",
    "\n",
    "# Langchain tut die Arbeit f√ºr uns...\n",
    "evaluator = load_evaluator(\"embedding_distance\", embeddings=get_embeddings(\"openai\"))\n",
    "distances = []\n",
    "for item in test_set:\n",
    "    distance = evaluator.evaluate_strings(prediction=item, reference=reference)\n",
    "    distances.append(distance[\"score\"])\n",
    "\n",
    "# Als Tabelle ausdrucken\n",
    "df = pd.DataFrame({\n",
    "    \"Wort\": test_set,\n",
    "    \"Entfernung\": distances\n",
    "})\n",
    "renderers = {\n",
    " \"Entfernung\": BarRenderer(\n",
    "        horizontal_alignment=\"center\",\n",
    "        bar_color=ColorScale(min=0, max=1, scheme=\"viridis\"),\n",
    "        bar_value=LinearScale(min=0, max=1),\n",
    "    )\n",
    "}\n",
    "grid = DataGrid(df, base_column_size=250, renderers=renderers)\n",
    "grid.transform(\n",
    "    [\n",
    "        {\"type\": \"sort\", \"columnIndex\": 2, \"desc\": False},\n",
    "    ]\n",
    ")\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Debug Informationen gew√ºnscht?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug, set_verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Und jetzt selber mal Ausprobieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.run({\n",
    "    'beruf': \"Programmierer\",\n",
    "    'ort': \"Stuttgart\"\n",
    "    }))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
