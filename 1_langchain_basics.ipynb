{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e218d63d-0e47-4ad3-90ef-03d085ffa77f",
   "metadata": {},
   "source": [
    "# ü¶úüîó Langchain Demo\n",
    "\n",
    "Welcome! This example will give you a first look at langchain. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e50e21",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051dad0-71d4-4c77-a382-037ffe7e6f32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab7ed6-9ab1-449c-bd8d-26da0422472b",
   "metadata": {},
   "source": [
    "## Load Environment\n",
    "\n",
    "Load ENV Variables from .env file\n",
    "\n",
    "needed ENV vars:\n",
    "\n",
    "```\n",
    "# OpenAI Hosted Variante\n",
    "OPENAI_API_KEY\n",
    "OPENAI_ORGANIZATION (optional)\n",
    "\n",
    "# Azure Hosted Variante\n",
    "OPENAI_API_KEY=\"\"\n",
    "OPENAI_API_BASE=\"https://<your instance name>.openai.azure.com/\"\n",
    "OPENAI_API_TYPE=\"azure\"\n",
    "OPENAI_API_VERSION= \"2023-05-15\"\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c066b0f-3a32-457b-a73f-471b4c74e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc660189-9ee6-45b6-80f6-95abcdf0b145",
   "metadata": {},
   "source": [
    "## Erster Test\n",
    "\n",
    "ChatGPT oder auch jedes andere LLM benutzen ist relativ einfach mit Langchain\n",
    "\n",
    "In diesen Test nutzen wir das \"gpt-3.5-turbo\" model - m√∂gliche Large Language Modele von OpenAI sind:\n",
    "- `gpt-3.5-turbo` // das model das am g√ºnstigsten und dadurch auch extrem verbreitet ist\n",
    "- `gpt-3.5-turbo-16k` // das selbe model nur mit einem viel gr√∂√üeren \"Ged√§chtnis\"\n",
    "- `gpt-4` // das neue und bessere GPT Model\n",
    "- `gpt-4-32k` // wie das andere GPT-4 Model nur auch wieder mit einem 4 mal so gro√üen \"Ged√§chtnis\"\n",
    "- Demn√§chst wohl auch gpt-4-turbo\n",
    "\n",
    "Diese Modelle gibt es auch alle noch einmal mit einem Zeitstempel, denn OpenAI bringt immer wieder neu trainierte Versionen der vorhandenen Modelle heraus.\n",
    "Dies kann auch dazu f√ºhren, das Anfragen an das LLM nun komplett andere Antworten geben. \n",
    "\n",
    "Wenn man hier nicht aufpasst, kann es dazu f√ºhren, das Programme, die auf diesen Modellen basieren, nicht mehr funktionieren,\n",
    "weil die passenden Marker im Text fehlen.\n",
    "\n",
    "### Temperatur\n",
    "Alle LLMs sind nicht deterministisch. Aber die Temperatur ist ein Parameter, mit der man die Variabilit√§t von Antworten hoch und runter schrauben kann.\n",
    "Wie bei normalen Atomen, wenn die Temperatur niedrig ist, ist die Bewegung niedrig, wenn man die Temperatur hochschraubt, wird viel gewackelt.\n",
    "Hier ist der Parameter ein Flie√ükommawert zwischen 0 und 1.\n",
    "\n",
    "### Links:\n",
    "- https://www.langchain.com/\n",
    "- https://python.langchain.com/docs/get_started/introduction\n",
    "- https://platform.openai.com/docs/models/gpt-3-5\n",
    "- https://platform.openai.com/docs/models/gpt-4\n",
    "- https://platform.openai.com/docs/deprecations\n",
    "- https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac177b4-e0fa-460a-9a79-d7c6cc7bb8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(azure_deployment=\"GPT3\")\n",
    "# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "print(llm.predict(\"Hi OpenAI! Kannst Du mir gerade mal einen plattdeutschen Trinkspruch kreiren?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d1837b-cd6c-4d0a-b6dd-e1bac0f49ad5",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "Token sind die kleinste Einheit mit der LLMs arbeiten.\n",
    "LLMs sind grob gesagt Wahrscheinlichkeitsmodele die das Token ausgeben das am wahrschneinlichsten zum vorherigen passt.\n",
    "Tokens k√∂nnen W√∂rter, machmal sogar Wortgruppen oder auch nur einzelne oder mehrer Buchstaben.\n",
    "\n",
    "So generieren LLMs die wahrschneinlichste fortf√ºhrung der Eingabetoken\n",
    "Its Statistik all the Way down\n",
    "\n",
    "Links:\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d606115d-f896-4c55-a416-3e9728f4eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "tokens = encoding.encode(\"AI ist eine tolle Sache.\")\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "decoded_tokens = [encoding.decode_single_token_bytes(token) for token in tokens]\n",
    "\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e9e9b-1299-4f66-8868-edea7eff96f1",
   "metadata": {},
   "source": [
    "## Prompt Engineering und Templates in Langchain\n",
    "\n",
    "Um die dinge von der AI zu bekommen die man wirklich will muss man oft sehr spezifisch Nachfragen und diese Nachfrage Templates\n",
    "k√∂nnen wir selber erstellen oder im falle von Langchain auf Langchain Hub nachschauen und benutzen\n",
    "\n",
    "Wenn man immer wieder und wieder den gleichen Text nur mit anderen Parametern haben will, kann man diese relativ einfach als PromptTemplate erstellen\n",
    "und diese an das LLM √ºbergeben.\n",
    "\n",
    "### Links\n",
    "- https://python.langchain.com/docs/get_started/quickstart#prompt-templates\n",
    "- https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering\n",
    "- https://learnprompting.org/docs/intro\n",
    "- https://www.promptingguide.ai/\n",
    "- https://smith.langchain.com/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd72dfa-3212-4b4a-a9b3-a2993c9c0b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Du bist ein {beruf} aus Sankt Pauli.\"),\n",
    "        (\"human\", \"Erkl√§r in 2 S√§tzen in hamburger dialekt warum Deine Kunden aus {ort} die besten sind.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt.format(beruf=\"B√§cker\", ort=\"Pinneberg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c92bcce-795b-44c8-ac37-5933f53f94aa",
   "metadata": {},
   "source": [
    "### Langchain Hub Beispiel\n",
    "\n",
    "Links:\n",
    "- https://smith.langchain.com/hub/takatost/conversation-title-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8461f1fa-388f-491a-8493-70e43ee764cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "obj = hub.pull(\"takatost/conversation-title-generator\")\n",
    "\n",
    "print(obj.format(question=\"Was kann ich mittels AI in meiner Firma automatisieren?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab465a-a468-4245-9ff3-891fe39944f8",
   "metadata": {},
   "source": [
    "## Die Chains die Langchain den Namen geben\n",
    "\n",
    "\n",
    "### Links\n",
    "- https://python.langchain.com/docs/modules/chains/how_to/async_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5da617-3044-4248-b890-f7cd27091408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "runnable = prompt | llm | StrOutputParser()\n",
    "\n",
    "# einfacher aufruf\n",
    "print(runnable.invoke({\"beruf\":\"B√§cker\", \"ort\":\"Pinneberg\"}))\n",
    "\n",
    "# streaming aufruf\n",
    "# for chunk in runnable.stream({\"beruf\":\"B√§cker\", \"ort\":\"Pinneberg\"}):\n",
    "    # print(chunk, end=\"\", flush=True)\n",
    "    # time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ef5ba-588a-4b03-afec-b43a7ede7505",
   "metadata": {},
   "source": [
    "## Embeddings und Vectoren\n",
    "\n",
    "Embeddings sind eine m√∂glichkeit Texte in f√ºr CPU/GPUs besser verarbeitbare Vectoren umzuwandeln. Mit diesen Vectorn kann man dann so sch√∂ne dinge machen wie:\n",
    "- Suche (wobei die Ergebnisse nach Relevanz f√ºr eine Abfragezeichenfolge geordnet werden)\n",
    "- Clustering (wobei Textzeichenfolgen nach √Ñhnlichkeit gruppiert werden)\n",
    "- Empfehlungen (wobei Elemente mit zugeh√∂rigen Textzeichenfolgen empfohlen werden)\n",
    "- Anomalieerkennung (wobei Ausrei√üer mit geringem Zusammenhang identifiziert werden)\n",
    "- Klassifizierung (wobei Textzeichenfolgen nach ihrer √§hnlichsten Bezeichnung klassifiziert werden)\n",
    "\n",
    "Die Standard Anwendung w√§re hier das Umwandeln von Artikeln in einer Knowledge Datenbank in Vectoren, diese in einer Vectordatenbank zu hinterlegen und dann beim\n",
    "Befragen eines Chatbots dann die anhand der Frage die \"√§hnlichsten\" oder n√§chstgelegenden Dokumente zu finden und diese dann als Kontext dem Chatbot zum beantworten der Frage mitzugeben\n",
    "\n",
    "Embeddings machen dies vorallem m√∂glihch weil sie nicht auf genaue oder ungenaue Stringmatches arbeiten sondern wirklich eine Semantische n√§he zueinander finden.\n",
    "Ein Beispiel w√§re das im Englischen Queen und King semantisch extrem nah sind aber in einer Stringmatch suche nicht zusammen zu finden sein sollten.\n",
    "Das Ber√ºhmte Embedding Beispiel ist auch immer das \"King - Man + Woman ~= Queen\"\n",
    "\n",
    "Es gibt sehr viele Embedding Modelle die f√ºr alle m√∂glichen F√§lle optimiert sind. Sie gibt es in Einsprachig und Mehrsprachig. Es gibt aber auch Multimodale Embeddings die z.B. f√ºr das Wort Schraube und das Bild einer Schraube sehr √§hnliche Vektoren herausgeben. Es gibt auch welche die auf Sprache trainiert wurden.\n",
    "\n",
    "Links:\n",
    "- https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "- https://python.langchain.com/docs/integrations/text_embedding/azureopenai\n",
    "- https://app.twelvelabs.io/blog/multimodal-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb9093e-4d20-45dd-9a00-7921615a099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings, AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(azure_deployment=\"Embeddings\")\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "\n",
    "query_result = embeddings.embed_query(text)\n",
    "\n",
    "print(\"Dimensions: \",len(query_result))\n",
    "\n",
    "# show the first 3 dimensions\n",
    "print(query_result[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee10063-7771-4dfc-9f79-9364e22778ec",
   "metadata": {},
   "source": [
    "### Einfaches Beispiel von Entfernungsberechnung mittels Vektoren\n",
    "Kleinere Zahl = N√§her an der Referenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d7c2c7-5b1f-4145-b95e-319f6880182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EmbeddingDistance, load_evaluator\n",
    "import pandas as pd\n",
    "\n",
    "from ipydatagrid import DataGrid, TextRenderer, BarRenderer, Expr\n",
    "from bqplot import LinearScale, ColorScale\n",
    "\n",
    "# Two lists of sentences\n",
    "reference = 'Schraube'\n",
    "\n",
    "sentences = [ 'Kreuzschlitz',\n",
    "              'Torx',\n",
    "              'Maggi',\n",
    "              'Inbus',\n",
    "              'Nagel',\n",
    "              'Tempo',\n",
    "            ]\n",
    "\n",
    "evaluator = load_evaluator(\"embedding_distance\", embeddings=embeddings)\n",
    "\n",
    "distances = []\n",
    "for sentence in sentences:\n",
    "    distance = evaluator.evaluate_strings(prediction=sentence, reference=reference)\n",
    "    distances.append(distance[\"score\"])\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Satz\": sentences,\n",
    "    \"Entfernung\": distances\n",
    "})\n",
    "\n",
    "renderers = {\n",
    " \"Entfernung\": BarRenderer(\n",
    "        horizontal_alignment=\"center\",\n",
    "        bar_color=ColorScale(min=0, max=1, scheme=\"viridis\"),\n",
    "        bar_value=LinearScale(min=0, max=1),\n",
    "    )\n",
    "}\n",
    "\n",
    "grid = DataGrid(df, base_column_size=250, renderers=renderers)\n",
    "grid.transform(\n",
    "    [\n",
    "        {\"type\": \"sort\", \"columnIndex\": 2, \"desc\": False},\n",
    "    ]\n",
    ")\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673d46c-321d-4724-9a9a-7db58f0cb88b",
   "metadata": {},
   "source": [
    "### Debug Informationen gew√ºnscht?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095caa2-8822-4954-8bf8-d629356d6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug, set_verbose\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916291da-750b-4d67-b694-c7eed809fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Und jetzt selber mal Ausprobieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186897d-df7a-4149-afd3-510ebdd911e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.run({\n",
    "    'beruf': \"Programmierer\",\n",
    "    'ort': \"Stuttgart\"\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f18e1-6ed7-4f05-a47f-ba552f8b39f2",
   "metadata": {},
   "source": [
    "Just go ahead and play a bit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
