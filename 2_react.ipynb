{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ü¶úüîó LangChain React Agent Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In diesem Part schauen wir uns Agenten an. Dabei nutzten wir die neueste Langchain Entwicklung, LangGraph. Mit LangGraph k√∂nnen Agenten als Graphen modelliert werden.\n",
    "\n",
    "Um die Komplexit√§t gering zu halten, verwenden wir in diesen Beispielen Prebuilt Agent- und Tool-Executors, die von Langchain bereit gestellt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Zuerst installieren wir uns die ben√∂tigten Pakete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Wir laden die ben√∂tigen Umgebungsvariablen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Jetzt definieren wir Tools die unser Agent benutzen kann. In diesem Fall eine Websuche √ºber DuckduckGo. Dadurch erlang der Agent die F√§higkeit Informationen aus dem Internet zu recherchieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolExecutor\n",
    "from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchRun\n",
    "\n",
    "tools = [DuckDuckGoSearchRun()]\n",
    "tools_executor = ToolExecutor(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Jetzt erstellen wir unseren Agenten. Hierf√ºr ben√∂tigen wir ein LLM mit dem der Agent ausgef√ºhrt werden soll. \n",
    "\n",
    "Wichtig ist hierbei, dass das Sprachmodell auf die Ausf√ºhrung von Agenten-Logik trainiert sein muss. Wir verwenden in diesem Beispiel die Modelle von OpenAI, da diese optimal f√ºr die Ausf√ºhrung von Agenten-Logik geeignet sind.\n",
    "\n",
    "Wie vorab erw√§hnt, nutzen wir einen Prebuilt Agent-Executor, in diesem Beispiel einen der OpenAI Functions ausf√ºhren kann.\n",
    "\n",
    "Nach Ausf√ºhrung des Agenten bekommen wir die einzelnen Schritte zur√ºck, die der Agent durchgef√ºhrt hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import chat_agent_executor\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from helpers import llm\n",
    "\n",
    "# app = chat_agent_executor.create_tool_calling_executor(llm, tools_executor)\n",
    "model = llm.with_config(configurable={\"llm\": \"azure\", \"temperature\": 1})\n",
    "app = chat_agent_executor.create_function_calling_executor(model, tools)\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Wann wurde Dieter Bohlen geboren? Wann seine Frau? Was ist die Altersdifferenz?\")]}\n",
    "\n",
    "for s in app.stream(inputs):\n",
    "    print(list(s.values()))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "In diesem Beispiel f√ºhren wir den zuvor definierten Agenten nochmals aus und streamen diesmal allerdings nur die finale Antwort des Agentenlauf raus. Dabei wird die Antwort Token f√ºr Token ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.prebuilt import chat_agent_executor\n",
    "from helpers import llm, graph_agent_llm_output_streamer\n",
    "\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Wann wurde Dieter Bohlen geboren? Wann seine Frau? Was ist die Altersdifferenz?\")]}\n",
    "\n",
    "model = llm.with_config(configurable={\"llm\": \"openai\", \"temperature\": 1})\n",
    "\n",
    "app = chat_agent_executor.create_function_calling_executor(model, tools)\n",
    "\n",
    "await graph_agent_llm_output_streamer(app, inputs)\n",
    "\n",
    "# async for output in app.astream_log(inputs, include_types=[\"llm\"]):\n",
    "#     # astream_log() yields the requested logs (here LLMs) in JSONPatch format\n",
    "#     for op in output.ops:\n",
    "#         if op[\"path\"] == \"/streamed_output/-\":\n",
    "#             # this is the output from .stream()\n",
    "#             # print(op)\n",
    "#             ...\n",
    "#         elif op[\"path\"].startswith(\"/logs/\") and op[\"path\"].endswith(\n",
    "#             \"/streamed_output/-\"\n",
    "#         ):\n",
    "#             # because we chose to only include LLMs, these are LLM tokens\n",
    "#             token_content = op[\"value\"].content\n",
    "#             if token_content:\n",
    "#                 print(op[\"value\"].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "TODO: Jetzt noch mit dem Tool executor, nicht mit function executor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
