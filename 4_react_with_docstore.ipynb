{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e095a2f-2b72-4ba8-b832-2ec162291162",
   "metadata": {},
   "source": [
    "# 🦜🔗 LangChain agent with docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8c85a3-40f9-4ec9-b934-4ad408804bcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.0.334)\n",
      "Requirement already satisfied: langchain-experimental in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (0.0.39)\n",
      "Requirement already satisfied: langchainhub in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.1.13)\n",
      "Requirement already satisfied: langchain-cli in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.0.16)\n",
      "Requirement already satisfied: langserve[all] in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.0.26)\n",
      "Requirement already satisfied: openai in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (1.2.3)\n",
      "Requirement already satisfied: python-dotenv in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: google-search-results in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (2.4.2)\n",
      "Requirement already satisfied: faiss-cpu in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (1.7.4)\n",
      "Requirement already satisfied: pypdf in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (3.17.0)\n",
      "Requirement already satisfied: tqdm in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (4.66.1)\n",
      "Requirement already satisfied: wikipedia in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (1.4.0)\n",
      "Requirement already satisfied: arxiv in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (2.0.0)\n",
      "Requirement already satisfied: atlassian-python-api in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (3.41.3)\n",
      "Requirement already satisfied: numexpr in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (2.8.7)\n",
      "Collecting tiktoken (from -r requirements.txt (line 16))\n",
      "  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/94/2f/0cc8fb3436d421d8fa2da370aca0283201f1b99e88a0f6e742bd8eef397d/tiktoken-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tiktoken-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.62 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (0.0.63)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (2.4.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (8.2.3)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchainhub->-r requirements.txt (line 3)) (2.31.0.10)\n",
      "Requirement already satisfied: gitpython<4.0.0,>=3.1.40 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain-cli->-r requirements.txt (line 4)) (3.1.40)\n",
      "Requirement already satisfied: tomlkit<0.13.0,>=0.12.2 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain-cli->-r requirements.txt (line 4)) (0.12.2)\n",
      "Requirement already satisfied: typer[all]<0.10.0,>=0.9.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain-cli->-r requirements.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: uvicorn<0.24.0,>=0.23.2 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langchain-cli->-r requirements.txt (line 4)) (0.23.2)\n",
      "Requirement already satisfied: fastapi>=0.90.1 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langserve[all]->-r requirements.txt (line 5)) (0.104.1)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langserve[all]->-r requirements.txt (line 5)) (0.25.1)\n",
      "Requirement already satisfied: httpx-sse>=0.3.1 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langserve[all]->-r requirements.txt (line 5)) (0.3.1)\n",
      "Requirement already satisfied: orjson>=2 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langserve[all]->-r requirements.txt (line 5)) (3.9.10)\n",
      "Requirement already satisfied: sse-starlette<2.0.0,>=1.3.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from langserve[all]->-r requirements.txt (line 5)) (1.6.5)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from openai->-r requirements.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from openai->-r requirements.txt (line 6)) (4.8.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from wikipedia->-r requirements.txt (line 12)) (4.12.2)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from arxiv->-r requirements.txt (line 13)) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from feedparser==6.0.10->arxiv->-r requirements.txt (line 13)) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: deprecated in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from atlassian-python-api->-r requirements.txt (line 14)) (1.2.14)\n",
      "Requirement already satisfied: six in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from atlassian-python-api->-r requirements.txt (line 14)) (1.16.0)\n",
      "Requirement already satisfied: oauthlib in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from atlassian-python-api->-r requirements.txt (line 14)) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from atlassian-python-api->-r requirements.txt (line 14)) (1.3.1)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->-r requirements.txt (line 16))\n",
      "  Obtaining dependency information for regex>=2022.1.18 from https://files.pythonhosted.org/packages/f2/b8/b1ec82fce93064a73ba67f2bb158ec9cac4a0e8f0b6942268ec963947329/regex-2023.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from anyio<4.0->langchain->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 1)) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from fastapi>=0.90.1->langserve[all]->-r requirements.txt (line 5)) (0.27.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from gitpython<4.0.0,>=3.1.40->langchain-cli->-r requirements.txt (line 4)) (4.0.11)\n",
      "Requirement already satisfied: httpcore in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]->-r requirements.txt (line 5)) (1.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain->-r requirements.txt (line 1)) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from pydantic<3,>=1->langchain->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from pydantic<3,>=1->langchain->-r requirements.txt (line 1)) (2.10.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli->-r requirements.txt (line 4)) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli->-r requirements.txt (line 4)) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli->-r requirements.txt (line 4)) (13.6.0)\n",
      "Requirement already satisfied: h11>=0.8 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from uvicorn<0.24.0,>=0.23.2->langchain-cli->-r requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from beautifulsoup4->wikipedia->-r requirements.txt (line 12)) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from deprecated->atlassian-python-api->-r requirements.txt (line 14)) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.40->langchain-cli->-r requirements.txt (line 4)) (5.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<0.10.0,>=0.9.0->langchain-cli->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<0.10.0,>=0.9.0->langchain-cli->-r requirements.txt (line 4)) (2.16.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/johann/anaconda3/envs/langchain_agents/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<0.10.0,>=0.9.0->langchain-cli->-r requirements.txt (line 4)) (0.1.2)\n",
      "Downloading tiktoken-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, tiktoken\n",
      "Successfully installed regex-2023.10.3 tiktoken-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "781f2be3-fa34-48f9-ac51-a6316efb2b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a664300-8704-4965-9dc3-bcf9dfddb00b",
   "metadata": {},
   "source": [
    "Wir haben die deutsche Variante davon erstellt, wer Interesse hat: [Download hier](http://hub.mayflower.de/langchain-buch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f233aca1-34e3-4b00-873d-1f7dae6324a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Too many inputs. The max number of inputs is 16.  We hope to increase the number of inputs per request soon. Please contact us through an Azure support request at: https://go.microsoft.com/fwlink/?linkid=2213926 for further questions.', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/data/src/ml/codetalks23/4_react_with_docstore.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/data/src/ml/codetalks23/4_react_with_docstore.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m texts \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mload_and_split()\n\u001b[1;32m      <a href='vscode-notebook-cell:/data/src/ml/codetalks23/4_react_with_docstore.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m embeddings \u001b[39m=\u001b[39m AzureOpenAIEmbeddings(azure_deployment\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEmbeddings\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/data/src/ml/codetalks23/4_react_with_docstore.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m db \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39;49mfrom_documents(texts, embeddings)\n\u001b[1;32m      <a href='vscode-notebook-cell:/data/src/ml/codetalks23/4_react_with_docstore.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m db\u001b[39m.\u001b[39msave_local(\u001b[39m\"\u001b[39m\u001b[39m./vectorstore\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_agents/lib/python3.11/site-packages/langchain/schema/vectorstore.py:510\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    509\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m--> 510\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39;49mmetadatas, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_agents/lib/python3.11/site-packages/langchain/vectorstores/faiss.py:911\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    885\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_texts\u001b[39m(\n\u001b[1;32m    886\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    892\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FAISS:\n\u001b[1;32m    893\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \n\u001b[1;32m    895\u001b[0m \u001b[39m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[39m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     embeddings \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39;49membed_documents(texts)\n\u001b[1;32m    912\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__from(\n\u001b[1;32m    913\u001b[0m         texts,\n\u001b[1;32m    914\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    919\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_agents/lib/python3.11/site-packages/langchain/embeddings/openai.py:549\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m engine \u001b[39m=\u001b[39m cast(\u001b[39mstr\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeployment)\n\u001b[0;32m--> 549\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_agents/lib/python3.11/site-packages/langchain/embeddings/openai.py:425\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    422\u001b[0m     _iter \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(tokens), _chunk_size)\n\u001b[1;32m    424\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m _iter:\n\u001b[0;32m--> 425\u001b[0m     response \u001b[39m=\u001b[39m embed_with_retry(\n\u001b[1;32m    426\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m         \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtokens[i : i \u001b[39m+\u001b[39;49m _chunk_size],\n\u001b[1;32m    428\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_invocation_params,\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    431\u001b[0m         response \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mdict()\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_agents/lib/python3.11/site-packages/langchain/embeddings/openai.py:106\u001b[0m, in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m _is_openai_v1():\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    107\u001b[0m retry_decorator \u001b[39m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[1;32m    109\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_embed_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_agents/lib/python3.11/site-packages/openai/resources/embeddings.py:105\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m         embedding\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfrombuffer(  \u001b[39m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    100\u001b[0m             base64\u001b[39m.\u001b[39mb64decode(data), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         )\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\n\u001b[0;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    106\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/embeddings\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    107\u001b[0m     body\u001b[39m=\u001b[39;49mmaybe_transform(params, embedding_create_params\u001b[39m.\u001b[39;49mEmbeddingCreateParams),\n\u001b[1;32m    108\u001b[0m     options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    109\u001b[0m         extra_headers\u001b[39m=\u001b[39;49mextra_headers,\n\u001b[1;32m    110\u001b[0m         extra_query\u001b[39m=\u001b[39;49mextra_query,\n\u001b[1;32m    111\u001b[0m         extra_body\u001b[39m=\u001b[39;49mextra_body,\n\u001b[1;32m    112\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    113\u001b[0m         post_parser\u001b[39m=\u001b[39;49mparser,\n\u001b[1;32m    114\u001b[0m     ),\n\u001b[1;32m    115\u001b[0m     cast_to\u001b[39m=\u001b[39;49mCreateEmbeddingResponse,\n\u001b[1;32m    116\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_agents/lib/python3.11/site-packages/openai/_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1042\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1043\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1052\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1053\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[0;32m-> 1055\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_agents/lib/python3.11/site-packages/openai/_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    826\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    833\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    835\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    836\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    837\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    838\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    839\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    840\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_agents/lib/python3.11/site-packages/openai/_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    875\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 877\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    879\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Too many inputs. The max number of inputs is 16.  We hope to increase the number of inputs per request soon. Please contact us through an Azure support request at: https://go.microsoft.com/fwlink/?linkid=2213926 for further questions.', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"langchain.pdf\")\n",
    "texts = loader.load_and_split()\n",
    "embeddings = AzureOpenAIEmbeddings(azure_deployment=\"Embeddings\")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "db.save_local(\"./vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5bce2d53-f548-489b-af2f-6c7e928967cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(azure_deployment=\"Embeddings\")\n",
    "db = FAISS.load_local(\"./vectorstore\", embeddings)\n",
    "retriever = db.as_retriever()\n",
    "llm = ChatOpenAI(temperature = 0) \n",
    "llm2 = AzureChatOpenAI(deployment_name=\"gpt-35-turbo\", \n",
    "                       api_version=\"2023-07-01-preview\",\n",
    "                       temperature = 0) \n",
    "tool = create_retriever_tool(\n",
    "    retriever, \n",
    "    \"search_langchain\",\n",
    "    \"Searches and returns information about langchain and vectorstores.\"\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "87870039-bffc-4420-a2e8-01efc5dbd311",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_conversational_retrieval_agent(llm2, tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "38dc38bb-1ec6-4b0f-8881-8b385602822a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_langchain` with `vectorstore`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='Llama-Index/GPT-IndexCaseStudy 25\\nas Wikipedia. LlamaIndex creates a vectorized index from your\\ndocument data, making it highly efficient to query. It then uses\\nthis index to identify the most relevant sections of the document\\nbasedonthequery.\\nLlamaIndex is useful because it provides a central interface to\\nconnectyourLLM’swithexternaldataandoffersdataconnectors\\ntoyourexistingdatasourcesanddataformats(API’s,PDF’s,docs,\\nSQL, etc.). It provides a simple, flexible interface between your\\nexternaldataandLLMs.\\nSome projects that use LlamaIndex include building personal as-\\nsistantswithLlamaIndexandGPT-3.5,usingLlamaIndexfordocu-\\nmentretrieval,andcombininganswersacrossdocuments.', metadata={'source': 'langchain.pdf', 'page': 29}), Document(page_content='UsingLLMsToOrganizeInformationinOurGoogleDrives 52\\n14 , the Council of Europe include all forms of physical exe\\n15 rcise, including those completed just for fun.\\nGoogle Drive Example Wrap Up\\nIf you already use Google Drive to store your working notes\\nand other documents, then you might want to expand the simple\\nexample in this chapter to build your own query system for your\\ndocuments.InadditiontoGoogleDrive,IalsouseMicrosoftOffice\\n365andOneDriveinmyworkandpersonalprojects.\\nI haven’t written my own connectors yet for OneDrive but this is\\nonmypersonalto-dolistusingtheMicrosoftlibrary https://github.\\ncom/OneDrive/onedrive-sdk-python .', metadata={'source': 'langchain.pdf', 'page': 56}), Document(page_content='Using LLMs To Organize\\nInformation in Our\\nGoogle Drives\\nMy digital life consists of writing, working as an AI practitioner,\\nandlearningactivitiesthatIjustifywithmyself-imageofa“gentle-\\nmanscientist.”CloudstoragelikeGitHub,GoogleDrive,Microsoft\\nOneDrive,andiCloudarecentraltomyactivities.\\nAbouttenyearsagoIspenttwomonthsofmytimewritingasystem\\nin Clojure that was planned to be my own custom and personal\\nDropBox,augmentedwithvariousNLPtoolsandaFireFoxplugin\\ntosendwebclippingsdirectlytomypersonalsystem.Tobehonest,\\nI stopped using my own project after a few months because the\\ntimeittooktoorganizemyinformationwasagreateropportunity\\ncostthanthevalueIreceived.\\nIn this chapter I am going to walk you through parts of a new\\nsystem that I am developing for my own personal use to help\\nme organize my material on Google Drive (and eventually other\\ncloud services). Don’t be surprised if the completed project is an\\nadditionalexampleinafutureeditionofthisbook!\\nWiththeGooglesetupdirectionslistedbelow,youwillgetapop-up\\nweb browsing window with a warning like (this shows my Gmail\\naddress, you should see your own Gmail address here assuming\\nthat you have recently logged into Gmail using your default web\\nbrowser):', metadata={'source': 'langchain.pdf', 'page': 49}), Document(page_content='ExamplesUsingHuggingFaceOpenSourceModels 69\\nUsingaastandard ColabGPU,thequery/prediction timeismuch\\nfaster.Hereisa linktomyColabnotebook ⁴ifyouwouldpreferto\\nrunthisexampleonColabinsteadofonyourlaptop.\\n⁴https://colab.research.google.com/drive/1Ecg-0iid3AD05zM4HgPXTVHcgkGxyi3q?usp=\\nsharing', metadata={'source': 'langchain.pdf', 'page': 73})]\u001b[0m\u001b[32;1m\u001b[1;3mTo use langchain with a vectorstore, you can follow these steps:\n",
      "\n",
      "1. Install and set up the langchain library.\n",
      "2. Create or obtain a vectorstore that contains the data you want to work with.\n",
      "3. Use langchain to interact with the vectorstore and perform various operations such as querying, retrieving, and manipulating the data.\n",
      "\n",
      "Langchain provides a central interface to connect your language models (LLMs) with external data sources and offers data connectors to your existing data sources and formats, such as APIs, PDFs, documents, SQL, etc. It simplifies the process of integrating and working with data in your LLMs.\n",
      "\n",
      "For more specific instructions and examples, you can refer to the documentation or examples provided by langchain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'How do i use langchain with a vectorstore?', 'chat_history': [HumanMessage(content='How do i use langchain with a vectorstore?'), AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"__arg1\": \"vectorstore\"\\n}', 'name': 'search_langchain'}}), FunctionMessage(content=\"[Document(page_content='Llama-Index/GPT-IndexCaseStudy 25\\\\nas Wikipedia. LlamaIndex creates a vectorized index from your\\\\ndocument data, making it highly efficient to query. It then uses\\\\nthis index to identify the most relevant sections of the document\\\\nbasedonthequery.\\\\nLlamaIndex is useful because it provides a central interface to\\\\nconnectyourLLM’swithexternaldataandoffersdataconnectors\\\\ntoyourexistingdatasourcesanddataformats(API’s,PDF’s,docs,\\\\nSQL, etc.). It provides a simple, flexible interface between your\\\\nexternaldataandLLMs.\\\\nSome projects that use LlamaIndex include building personal as-\\\\nsistantswithLlamaIndexandGPT-3.5,usingLlamaIndexfordocu-\\\\nmentretrieval,andcombininganswersacrossdocuments.', metadata={'source': 'langchain.pdf', 'page': 29}), Document(page_content='UsingLLMsToOrganizeInformationinOurGoogleDrives 52\\\\n14 , the Council of Europe include all forms of physical exe\\\\n15 rcise, including those completed just for fun.\\\\nGoogle Drive Example Wrap Up\\\\nIf you already use Google Drive to store your working notes\\\\nand other documents, then you might want to expand the simple\\\\nexample in this chapter to build your own query system for your\\\\ndocuments.InadditiontoGoogleDrive,IalsouseMicrosoftOffice\\\\n365andOneDriveinmyworkandpersonalprojects.\\\\nI haven’t written my own connectors yet for OneDrive but this is\\\\nonmypersonalto-dolistusingtheMicrosoftlibrary https://github.\\\\ncom/OneDrive/onedrive-sdk-python .', metadata={'source': 'langchain.pdf', 'page': 56}), Document(page_content='Using LLMs To Organize\\\\nInformation in Our\\\\nGoogle Drives\\\\nMy digital life consists of writing, working as an AI practitioner,\\\\nandlearningactivitiesthatIjustifywithmyself-imageofa“gentle-\\\\nmanscientist.”CloudstoragelikeGitHub,GoogleDrive,Microsoft\\\\nOneDrive,andiCloudarecentraltomyactivities.\\\\nAbouttenyearsagoIspenttwomonthsofmytimewritingasystem\\\\nin Clojure that was planned to be my own custom and personal\\\\nDropBox,augmentedwithvariousNLPtoolsandaFireFoxplugin\\\\ntosendwebclippingsdirectlytomypersonalsystem.Tobehonest,\\\\nI stopped using my own project after a few months because the\\\\ntimeittooktoorganizemyinformationwasagreateropportunity\\\\ncostthanthevalueIreceived.\\\\nIn this chapter I am going to walk you through parts of a new\\\\nsystem that I am developing for my own personal use to help\\\\nme organize my material on Google Drive (and eventually other\\\\ncloud services). Don’t be surprised if the completed project is an\\\\nadditionalexampleinafutureeditionofthisbook!\\\\nWiththeGooglesetupdirectionslistedbelow,youwillgetapop-up\\\\nweb browsing window with a warning like (this shows my Gmail\\\\naddress, you should see your own Gmail address here assuming\\\\nthat you have recently logged into Gmail using your default web\\\\nbrowser):', metadata={'source': 'langchain.pdf', 'page': 49}), Document(page_content='ExamplesUsingHuggingFaceOpenSourceModels 69\\\\nUsingaastandard ColabGPU,thequery/prediction timeismuch\\\\nfaster.Hereisa linktomyColabnotebook ⁴ifyouwouldpreferto\\\\nrunthisexampleonColabinsteadofonyourlaptop.\\\\n⁴https://colab.research.google.com/drive/1Ecg-0iid3AD05zM4HgPXTVHcgkGxyi3q?usp=\\\\nsharing', metadata={'source': 'langchain.pdf', 'page': 73})]\", name='search_langchain'), AIMessage(content='To use langchain with a vectorstore, you can follow these steps:\\n\\n1. Install and set up the langchain library.\\n2. Create or obtain a vectorstore that contains the data you want to work with.\\n3. Use langchain to interact with the vectorstore and perform various operations such as querying, retrieving, and manipulating the data.\\n\\nLangchain provides a central interface to connect your language models (LLMs) with external data sources and offers data connectors to your existing data sources and formats, such as APIs, PDFs, documents, SQL, etc. It simplifies the process of integrating and working with data in your LLMs.\\n\\nFor more specific instructions and examples, you can refer to the documentation or examples provided by langchain.')], 'output': 'To use langchain with a vectorstore, you can follow these steps:\\n\\n1. Install and set up the langchain library.\\n2. Create or obtain a vectorstore that contains the data you want to work with.\\n3. Use langchain to interact with the vectorstore and perform various operations such as querying, retrieving, and manipulating the data.\\n\\nLangchain provides a central interface to connect your language models (LLMs) with external data sources and offers data connectors to your existing data sources and formats, such as APIs, PDFs, documents, SQL, etc. It simplifies the process of integrating and working with data in your LLMs.\\n\\nFor more specific instructions and examples, you can refer to the documentation or examples provided by langchain.', 'intermediate_steps': [(AgentActionMessageLog(tool='search_langchain', tool_input='vectorstore', log='\\nInvoking: `search_langchain` with `vectorstore`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"__arg1\": \"vectorstore\"\\n}', 'name': 'search_langchain'}})]), [Document(page_content='Llama-Index/GPT-IndexCaseStudy 25\\nas Wikipedia. LlamaIndex creates a vectorized index from your\\ndocument data, making it highly efficient to query. It then uses\\nthis index to identify the most relevant sections of the document\\nbasedonthequery.\\nLlamaIndex is useful because it provides a central interface to\\nconnectyourLLM’swithexternaldataandoffersdataconnectors\\ntoyourexistingdatasourcesanddataformats(API’s,PDF’s,docs,\\nSQL, etc.). It provides a simple, flexible interface between your\\nexternaldataandLLMs.\\nSome projects that use LlamaIndex include building personal as-\\nsistantswithLlamaIndexandGPT-3.5,usingLlamaIndexfordocu-\\nmentretrieval,andcombininganswersacrossdocuments.', metadata={'source': 'langchain.pdf', 'page': 29}), Document(page_content='UsingLLMsToOrganizeInformationinOurGoogleDrives 52\\n14 , the Council of Europe include all forms of physical exe\\n15 rcise, including those completed just for fun.\\nGoogle Drive Example Wrap Up\\nIf you already use Google Drive to store your working notes\\nand other documents, then you might want to expand the simple\\nexample in this chapter to build your own query system for your\\ndocuments.InadditiontoGoogleDrive,IalsouseMicrosoftOffice\\n365andOneDriveinmyworkandpersonalprojects.\\nI haven’t written my own connectors yet for OneDrive but this is\\nonmypersonalto-dolistusingtheMicrosoftlibrary https://github.\\ncom/OneDrive/onedrive-sdk-python .', metadata={'source': 'langchain.pdf', 'page': 56}), Document(page_content='Using LLMs To Organize\\nInformation in Our\\nGoogle Drives\\nMy digital life consists of writing, working as an AI practitioner,\\nandlearningactivitiesthatIjustifywithmyself-imageofa“gentle-\\nmanscientist.”CloudstoragelikeGitHub,GoogleDrive,Microsoft\\nOneDrive,andiCloudarecentraltomyactivities.\\nAbouttenyearsagoIspenttwomonthsofmytimewritingasystem\\nin Clojure that was planned to be my own custom and personal\\nDropBox,augmentedwithvariousNLPtoolsandaFireFoxplugin\\ntosendwebclippingsdirectlytomypersonalsystem.Tobehonest,\\nI stopped using my own project after a few months because the\\ntimeittooktoorganizemyinformationwasagreateropportunity\\ncostthanthevalueIreceived.\\nIn this chapter I am going to walk you through parts of a new\\nsystem that I am developing for my own personal use to help\\nme organize my material on Google Drive (and eventually other\\ncloud services). Don’t be surprised if the completed project is an\\nadditionalexampleinafutureeditionofthisbook!\\nWiththeGooglesetupdirectionslistedbelow,youwillgetapop-up\\nweb browsing window with a warning like (this shows my Gmail\\naddress, you should see your own Gmail address here assuming\\nthat you have recently logged into Gmail using your default web\\nbrowser):', metadata={'source': 'langchain.pdf', 'page': 49}), Document(page_content='ExamplesUsingHuggingFaceOpenSourceModels 69\\nUsingaastandard ColabGPU,thequery/prediction timeismuch\\nfaster.Hereisa linktomyColabnotebook ⁴ifyouwouldpreferto\\nrunthisexampleonColabinsteadofonyourlaptop.\\n⁴https://colab.research.google.com/drive/1Ecg-0iid3AD05zM4HgPXTVHcgkGxyi3q?usp=\\nsharing', metadata={'source': 'langchain.pdf', 'page': 73})])]}\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor({\"input\": \"How do i use langchain with a vectorstore?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
